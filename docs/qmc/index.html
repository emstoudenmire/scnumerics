<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>SC Numerics</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/scnumerics/css/normalize.css">
  <link rel="stylesheet" href="/scnumerics/css/skeleton.css">
  <link rel="stylesheet" href="/scnumerics/css/style.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: 
            {
            Macros: 
                { 
                avg: ["{\\langle #1\\rangle}",1],
                iprod: ["{\\langle #1, #2 \\rangle}",2],
                bra: ["{\\langle #1|}",1],
                ket: ["{|#1\\rangle}",1]
                },
            equationNumbers: { autoNumber: "AMS" }
            },
        tex2jax: 
            {
            inlineMath: [['\$','\$'] ], 
            displayMath: [ ['@@','@@'] ]
            },
        "HTML-CSS": {scale: 100}
        });
  </script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML"></script>

</head>
<body style="background-color:#FFF;">

 <div class="container">
 <div class="row" style="margin-top: 2%">

    <span class="twelve columns">
        <table style="width:100%;">
        <tr>
        <td style="width:10%;">
            <!--
        <a href="/"><img style="height:100%; max-height: 40px; padding:0; margin-top: 5px; margin-right:10px; vertical-align: middle;" src="/images/tn_logo.png"/></a>
            -->
        </td>
        <td style="width:90%;" class="top_navbar"> 
           &nbsp;
           <a href="/scnumerics">Main</a>
           <a href="/scnumerics/about/">About</a>
           <!--<a href="/contribute/">Contribute</a>-->
           <a target="_blank" href="https://github.com/emstoudenmire/scnumerics">Source</a>
        </td>
        </tr>
<tr><td></td><td class='backlinks'>
<a href='/'>main</a>/qmc/</td></tr>          </table>
    </span> <!-- twelve columns -->

  </div> <!-- row -->
  <br/>
  <!--  </div> --> <!-- container -->

 <div class="row" style="margin-top: 2%">
</div> <!-- class="row" -->
<h1>Quantum Monte Carlo (QMC)</h1>
<div class="toc">
<b>Table of Contents</b><br/><br/>
    <ul><li><a href="#toc_1">Statement of the Problem</a></li>
    <li><a href="#toc_2">Steps of the DMRG Algorithm</a></li>
      <ul><li><a href="#toc_3">Step 0: Setup</a></li>
      <li><a href="#toc_4">Step 1a: Optimization of First Bond</a></li>
      <li><a href="#toc_5">Step 1b: Adaptive Restoration of MPS Form</a></li>
      <li><a href="#toc_6">Step 2: Optimization and Adaptation of Second Bond</a></li>
      <li><a href="#toc_7">Remaining Steps And DMRG Sweeping Algorithm</a></li>
    </ul><li><a href="#toc_8">Diagrammatic Summary of Main Steps</a></li>
    <li><a href="#toc_9">Convergence Properties</a></li>
    <li><a href="#toc_10">DMRG for Tree Tensor Networks</a></li>
    <li><a href="#toc_11">Connections to Other Algorithms</a></li>
    <li><a href="#toc_12">References</a></li>
</ul></div>
<p>Quantum Monte Carlo is a
In physics and chemistry applications, DMRG is mainly used to find ground states of
Hamiltonians of many-body quantum systems. It has also been extended to
compute excited states, and to simulate
dynamical, finite-temperature, and non-equilibrium systems.
<a name="toc_1"></a></p>
<h2>Statement of the Problem</h2>
<p>Consider a Hermitian matrix $H$ acting in vector space that
is the tensor product of $N$ smaller spaces, each of dimension $d$:</p>
<p><img src="H_diagram.png" alt="medium" /></p>
<p>The DMRG algorithm seeks the dominant eigenvector of $H$ in the form of an MPS tensor network</p>
<p><img src="H_eigenvector.png" alt="medium" /></p>
<p>Here $E_0$  $(\leq E_1 \leq E_2 \ldots)$ is the minimum eigenvalue of $H$.
(See below for a discussion of what the DMRG algorithm does when $H$ has more
than one minimum eigenvalue.)</p>
<p>For the algorithm to be efficient, $H$ must have certain simplifying properties.
For example $H$ could be a sum of local terms</p>
<p><img src="H_local_terms.png" alt="medium" /></p>
<p>or, more generally, $H$ could be given as an <a href="unknown_file">MPO</a> tensor network</p>
<p><img src="H_MPO_form.png" alt="medium" /></p>
<p>The MPO form is the most natural one for the DMRG algorithm, and can efficiently
represent many cases one wants to consider, such as when $H$ is a sum of local
terms.</p>
<p>However, other simplifying forms of $H$ can also permit efficient formulations of the DMRG
algorithm, such as if $H$ is a sum of MPO tensor networks or outer products of MPS
tensor networks.
<a name="toc_2"></a></p>
<h2>Steps of the DMRG Algorithm</h2>
<p><a name="toc_3"></a></p>
<h3>Step 0: Setup</h3>
<p>Before beginning the DMRG algorithm, it is imperative to bring the initial MPS into an orthogonal form via a <a href="unknown_file">gauge transformation</a>. Here we will choose to begin the DMRG algorithm assuming (without loss of generality) that the MPS tensors 2,3,...,N are all right-orthogonal:</p>
<p><img src="right_ortho_MPS.png" alt="medium" /></p>
<p>Because of the right-orthogonality property, we can interpret the MPS tensors numbers
$3,4,5,\ldots$ collectively as a change of basis from the basis of visible indices
$i_3,i_4,i_5,\ldots$ to the bond index $\alpha_2$ as follows:</p>
<p><img src="MPS_change_of_basis.png" alt="medium" /></p>
<p>This interpretation motivates transforming the matrix $H$ into the $i_1,i_2,\alpha_2$
basis as given by the following diagram:</p>
<p><img src="projected_H.png" alt="medium" /></p>
<p>If we take $H$ to be in <a href="unknown_file">MPO</a> form, we can compute the transformation efficiently,
defining the $R_j$ tensors along the way:</p>
<p><img src="H_edge.png" alt="medium" /></p>
<p>For efficiency, it is crucial that the edge tensors be created by contracting
each MPS or MPO tensor one at a time in a certain order, as follows:</p>
<p><img src="H_edge_ordering.png" alt="medium" /></p>
<p>Next, combine the first two MPS tensors by contracting over their shared bond index:</p>
<p><img src="bond1.png" alt="medium" /></p>
<p>At the point, it is helpful to observe that the &quot;bond tensor&quot; $B^{i_1 i_2 \alpha}_{12}$ does not contain just partial information about the tensor represented by the MPS. Rather, it is the <em>entire</em> tensor (eigenvector of $H$) just written in the $i_1, i_2, \alpha_2$ basis. If the $\alpha_2$ basis spans a subspace of the $i_3, i_4, \ldots, i_N$ space sufficient to represent the eigenvector of $H$ we seek, then optimizing $B_{12}$ to be the dominant eigenvector of the transformed $H$ is sufficient to solve the original problem.</p>
<p>In practice, however the $\alpha_2$ basis can be improved, and to do so the algorithm optimizes the MPS at every bond sequentially.
<a name="toc_4"></a></p>
<h3>Step 1a: Optimization of First Bond</h3>
<p>To optimize the first bond tensor $B_{12}$, a range of procedures can be used---for example, gradient descent or conjugate gradient---but among the most effective are iterative algorithms, such as the Davidson<a class="citation" href="#Davidson:1975_1">[1]</a> or Lanczos<a class="citation" href="#LanczosWikipedia_2">[2]</a><a class="citation" href="#Lanczos:1950_3">[3]</a> algorithms.</p>
<p>The key step of each of these algorithms is the multiplication of $B_{12}$ by the (transformed) matrix $H$. Using the projected form of $H$ in terms of the $R_j$ tensors defined above, this multiplication can be computed as:</p>
<p><img src="B12_H_mult.png" alt="medium" /></p>
<p>where one must take care to contract the MPO and $R_3$ tensors one at a time and
in a reasonable order to ensure efficiency.</p>
<p>The iterative algorithm takes the resulting $H$ times $B_{12}$ tensor and
processes it further, depending on the particular algorithm used.
Multiplication steps of same form as the diagram above
are repeated until some convergence criterion is met. These multiplication steps
typically dominate the cost of a DMRG calculation. The result is an improved
bond tensor $B^\prime_{12}$ which more closely approximates the dominant eigenvector
of $H$ one is seeking.</p>
<p>It is important to note that for an efficient DMRG algorithm, one should not fully
converge the iterative eigensolver algorithm at this step, or subsequent similar
bond-optimization steps.
Until the entire algorithm converges globally, the subspace spanned by the
rest of the MPS tensors cannot be assumed to be the ideal one for representing
the true eigenvector. It would be counterproductive to fully converge $B_{12}$
in such an imperfect subspace.
<a name="toc_5"></a></p>
<h3>Step 1b: Adaptive Restoration of MPS Form</h3>
<p>Having obtained an improved bond tensor $B^\prime_{12}$, it is necessary to
restore the MPS form of the candidate eigenvector before proceeding to
optimize the next bond. Otherwise, the algorithm would become inefficient.</p>
<p>The simplest way to optimally restore MPS form is to compute
a singular value decomposition (SVD) of the $B^\prime_{12}$ tensor:</p>
<p><img src="B12_svd.png" alt="medium" /></p>
<p>Crucially, one keeps only the first $m$ columns of $U_1$ and rows of $V_2$
corresponding to the $m$ largest singular values of $B^\prime_{12}$.
The other columns and rows of $U_1$ and $V_2$ are truncated, or discarded.</p>
<p>To complete the restoration of MPS form, one multiplies the singular values
into the matrix $V_2$ to obtain a new second MPS tensor $M_2 = S_{12} V_2$:</p>
<p><img src="SV_mult.png" alt="medium" /></p>
<p>Note that the new MPS tensor carrying the index $i_1$ will automatically
obey the left orthogonality property since it was formed from a unitary
matrix as guaranteed by the properties of the SVD:</p>
<p><img src="left_ortho.png" alt="medium" /></p>
<p>The SVD approach produces an optimal MPS in the sense of minimizing the
Euclidean distance between the MPS and the network with $B^\prime_{12}$
as the first tensor.</p>
<p>In other words, the distance:</p>
<p><img src="MPS_dist.png" alt="medium" /></p>
<p>is minimized, subject to the constraint that the resulting MPS bond dimension is $m$.
Because the tensors carrying the external indices $i_3, i_4, \ldots, i_N$ are
all right-orthogonal (and thus parameterize an orthonormal sub-basis), it
is sufficient for minimizing the above distance to just minimize the distance
between $B^\prime_{12}$ and the product of SVD factors $U_1 S_{12} V_2$.</p>
<p>An alternative, and formally equivalent, way to restore MPS form is
to form and then diagonalize the following <em>density matrix</em></p>
<p><img src="B12_denmat.png" alt="medium" /></p>
<p>The tensor $U_1$ obtained by diagonalizing the density matrix and truncating
according to its eigenvalues (which are the squares of the singular values $S_12$)
is identical to $U_1$ obtained from the SVD. The tensor $M_2$ can be obtained by
contracting $U_1$ with $B^\prime_{12}$.</p>
<p>It is important to mention the density matrix approach for two reasons. First,
it is the original approach used by White and the basis for the name
<em>density matrix renormalization group</em>, since the truncated diagonalization
was originally interpreted motivated as a kind of physical
<a href="https://en.wikipedia.org/wiki/Renormalization_group">renormalization group</a> procedure.
Secondly, the density matrix approach has the advantage that the density matrix
can be modified for various purposes before diagonalizing it to obtain the new MPS
tensors. One modification involves summing density matrices from multiple MPS
to obtain a basis suitable for representing them all in the so-called
<em>multiple targeting</em> procedure.<a class="citation" href="#Schollwoeck:2005_4">[4]</a> Another important modification is
the <em>noise term</em> perturbation often used to improve the
convergence properties of DMRG.<a class="citation" href="#White:2005_5">[5]</a> (However, also see
the more recent <em>subspace expansion</em> approach which uses the SVD
formalism.<a class="citation" href="#Hubig:2015_6">[6]</a>)
<a name="toc_6"></a></p>
<h3>Step 2: Optimization and Adaptation of Second Bond</h3>
<p>After merging and improving the first two MPS tensors, then restoring MPS form
the algorithm continues by carrying out a similar procedure for the MPS tensors
sharing the second bond index.</p>
<p>First one merges these two tensors to define the bond tensor $B_{23}$:</p>
<p><img src="B23.png" alt="medium" /></p>
<p>The previously obtained tensor $U_1$ is used to update the projection
of $H$ into the basis in which $B_{23}$ is defined:</p>
<p><img src="L1.png" alt="medium" /></p>
<p>Then $B_{23}$ is optimized via an iterative method, with the key step
being the multiplication of $H$ (in projected form) times $B_{23}$:</p>
<p><img src="mult_H_B23.png" alt="medium" /></p>
<p>Note that the tensor $R_4$ does not need to be recomputed for this
step, but was assumed to be saved in memory from the initial pass
of computing the $R_j$ tensors in Step 0 above.</p>
<p>After obtaining the improved bond tensor $B^\prime_{23}$, one
again factorizes it (using the truncated SVD, say) to restore
MPS form and adapt the bond dimension:</p>
<p><img src="B23_SVD.png" alt="medium" /></p>
<p>To prepare for the next step, one merges the singular value matrix $S_{23}$ with $V_3$
to define $M_3$:</p>
<p><img src="M3.png" alt="medium" /></p>
<p>and one uses the newly obtained $U_2$ tensor to update the projected form of $H$
by computing $L_2$ as follows:</p>
<p><img src="L2.png" alt="medium" />
<a name="toc_7"></a></p>
<h3>Remaining Steps And DMRG Sweeping Algorithm</h3>
<p>Following the optimization of the second two MPS tensors (corresponding to
the external indices $i_2$ and $i_3$), very similar steps can be carried
out to optimize the remaining MPS tensors two at a time and to adapt
all of the bond indices of the MPS. Once all tensors have been
optimized in a forward pass, one has done a single &quot;half sweep&quot; of DMRG.</p>
<p>Next one performs the procedure in reverse, merging and optimizing MPS
tensors $(N-1)$ and $N$ back to the first bond. After reaching the
first bond again, one has done a full sweep of DMRG.</p>
<p><a name="toc_8"></a></p>
<h2>Diagrammatic Summary of Main Steps</h2>
<p>To convey the bigger picture of what the DMRG algorithm does, it is
helpful to summarize the main steps diagrammatically.</p>
<p>At each bond one optimizes, the two MPS tensors sharing the bond
index are contracted together, then multiplied by $H$. To make
this multiplication efficient, $H$ is transformed into the basis
in which the bond tensor is defined by contracting $H$ with all of
the other MPS tensors:</p>
<p><img src="summary_opt.png" alt="medium" /></p>
<p>The full optimization of the bond tensor involves other steps as
well, depending on the particular eigensolver algorithm being used
(such as Davidson or Lanczos), but multiplication by $H$ is the most
costly and technical step.</p>
<p>After the bond tensor is optimized, it is factorized using a truncated
SVD to restore MPS form:</p>
<p><img src="summary_SVD.png" alt="medium" /></p>
<p>Here, in the last equation, one has multiplied the singular values into the right-hand SVD
factor tensor, intending to optimize the bond to the right next.</p>
<p>Now the process can be repeated for the next bond (to the right)
and all bonds of the MPS in turn.</p>
<!-- <a name="toc_9"></a>

## Convergence Properties

- Exponential convergence in sweeps when H is 1D, local, gapped / finite correlation length
- Not guaranteed to converge to dominant eigenvector: "sticking problem"
- Discuss case of degenerate minimum eigenvalues (MES hypothesis)
-->
<!-- <a name="toc_10"></a>

## DMRG for Tree Tensor Networks

The DMRG algorithm can be extended straightforwardly to arbitrary [tree tensor networks](unknown_file), of 
which MPS are a special case. Instead of sweeping forward and backward along an MPS chain, the
algorithm visits each bond of a tree, otherwise performing very similar optimization and factorization
steps. 

An important optimization compared to the MPS case, however, is that instead of merging adjacent
tree tensors by simply contracting their shared bond index, one first factorizes each factor tensor
into a tensor carrying the external index and a remainder, then merges the tensors carrying 
the external indices as shown below

![medium](ttn_dmrg_factor.png)

After optimizing the merged tensor, it is factorized using an SVD and the tree tensor network
form restored:

![medium](ttn_dmrg_restored.png)

-->
<!-- <a name="toc_11"></a>

## Connections to Other Algorithms

- An algorithm with similar steps as DMRG has been developed for (Discuss Rolfe EM algorithm)
-->
<p><a name="toc_refs"></a> <a name="toc_12"></a></p>
<h2>References</h2>
<ol>
<li><a name="Davidson:1975_1"></a><a href="https://doi.org/10.1016/0021-9991(75)90065-0"><em>The Iterative Calculation of a Few of the Lowest Eigenvalues and Corresponding Eigenvectors of Large Real-Symmetric Matrices</em></a>, Ernest R. Davidson, <i>J. Comput. Phys.</i> <b>17</b>, 87 (1975)</li>
<li><a name="LanczosWikipedia_2"></a><a href="https://en.wikipedia.org/wiki/Lanczos_algorithm"><em>Lanczos Algorithm</em></a>, <i>Wikipedia</i></li>
<li><a name="Lanczos:1950_3"></a><a href="http://dx.doi.org/10.6028/jres.045.026"><em>An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators</em></a>, Cornelius Lanczos, <i>Journal of Research of the National Bureau of Standards</i> <b>45</b>, 255--282 (1950)</li>
<li><a name="Schollwoeck:2005_4"></a><a href="http://link.aps.org/doi/10.1103/RevModPhys.77.259"><em>The density-matrix renormalization group</em></a>, U. Schollwöck, <i>Rev. Mod. Phys.</i> <b>77</b>, 259--315 (2005), <span>cond‑mat/<a href="https://arxiv.org/abs/cond-mat/0409292">0409292</a></span></li>
<li><a name="White:2005_5"></a><a href="http://dx.doi.org/10.1103/PhysRevB.72.180403"><em>Density matrix renormalization group algorithms with a single center site</em></a>, Steven R. White, <i>Phys. Rev. B</i> <b>72</b>, 180403 (2005), <span>cond‑mat/<a href="https://arxiv.org/abs/cond-mat/0508709">0508709</a></span></li>
<li><a name="Hubig:2015_6"></a><a href="http://link.aps.org/doi/10.1103/PhysRevB.91.155115"><em>Strictly single-site DMRG algorithm with subspace expansion</em></a>, C. Hubig, I.P. McCulloch, U. Schollwock, F. A. Wolf, <i>Phys. Rev. B</i> <b>91</b>, 155115 (2015), arxiv:<a href="https://arxiv.org/abs/1501.05504">1501.05504</a></li>
</ol>
  </div> <!-- class="container" -->

</body>
</html>
